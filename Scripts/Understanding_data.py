from utilities import *

# Get current directory
cwd = os.getcwd()
# Get parent directory
task_dir = os.path.abspath(os.path.join(cwd, os.pardir))
# Get data directory
data_dir = os.path.join(task_dir, "data")
# Select file at random
filename = os.path.join(data_dir, random.choice(os.listdir(data_dir)))

df = pd.read_csv(filename, encoding='unicode_escape', sep="\s|:", engine='python')
print("raw data - ")
print(df.info())

# Removing unnecessary columns
df = df.iloc[:, :4]

# Converting string values to float values
df = df.applymap(lambda x: float(x.replace(',', '.').replace('E', 'e')))
print("transformed data -")
print(df.info())

print("Distribution of data along different columns -")
print(df.describe())

# Getting critical points
cp2, cp3, cp4 = critical_points(df)

# Plotting impedance curve
impedance_curve(df, 'trial', cp2, cp3, cp4)

delta_t = find_delta_t(df, cp2, cp4)
print(f"Time difference between CP2 and CP4 for this measurement(delta_T) = {delta_t}")

xcp23 = find_train_data(df, cp2, cp3)

print("Distribution of data between CP2 and CP3 -")
print(xcp23.describe())

# There is huge difference between numerical values in all the columns,
# Hence, standardization will be required before training
xcp23 = xcp23.apply(stats.zscore)

# Plotting all columns to visualise data
columns = list(df.columns)
plt.scatter(df['time_/min'].iloc[cp2:cp3+1], xcp23['time_/min'], label=columns[0], s=5)
plt.scatter(df['time_/min'].iloc[cp2:cp3+1], xcp23['temp/Â°C'], label=columns[1], s=5)
plt.scatter(df['time_/min'].iloc[cp2:cp3+1], xcp23['DSCalpha'], label=columns[2], s=5)
plt.scatter(df['time_/min'].iloc[cp2:cp3+1], xcp23['impedance1.78kHz/Ohm'], label=columns[3], s=5)
plt.title('CP2 to CP3 (Standardized values)')
plt.ylabel('Z-Score')
plt.xlabel('Time (Minutes)')
plt.legend(loc='upper left')
plt.show()

# Plotting correlation plot to confirm the correlation between all the variables
sns.heatmap(xcp23.corr())
plt.show()

# Removing temperature values as its distribution between CP2 and CP3 should not affect the values of delta_t
# Because, it is almost constant for all the measurements
xcp23 = xcp23.iloc[:, 1:4]

# There is clear correlation between all the variables, so we can do dimensionality reduction using PCA
pca = PCA(n_components=3)
pca.fit(xcp23)
print(f"Weightage after new dimensions after applying PCA {pca.explained_variance_ratio_*100}")

# It is clear that first two dimensions represent complete data
# However we should check it for few more measurements to make sure if 2 dimensions are enough

print("Validating PCA dimensionality for 10 random cycles")
files = glob.glob(data_dir + "/*.csv")
files = random.sample(files, 10)

# loop over the list of csv files
for i, f in enumerate(files):
    # reading the csv file
    df = pd.read_csv(f, encoding='unicode_escape', sep="\s|:", engine='python')

    # Keeping only required columns and converting data to required format
    df = df.iloc[:, :4]
    df = df.applymap(lambda x: float(x.replace(',', '.').replace('E', 'e')))

    # Finding critical points and creating dataset
    cp2, cp3, cp4 = critical_points(df)
    xcp23 = find_train_data(df, cp2, cp3)

    # Standardizing all columns
    xcp23 = xcp23.apply(stats.zscore)
    xcp23 = xcp23.iloc[:, 1:4]

    # PCA analysis
    cycle = f.split('\\')[-1].split('.')[0]
    print(f"{i + 1}) For cycle {cycle}")
    pca = PCA(n_components=3)
    pca.fit(xcp23)
    print(f"{pca.explained_variance_ratio_ * 100}\n")

# From all the observations it is clear that, only 2 dimensions after PCA are sufficient to represent complete data

pca = PCA(n_components=2)
pca.fit(xcp23)
data_pca = pca.transform(xcp23)
data_pca = pd.DataFrame(data_pca, columns=["pca1", "pca2"])

# The time at which CP2 and CP3 is reached might have some effect on the time to reach CP4.
# Hence, the affect of the numerical value of time at CP2/CP3 must be retained
# We can scale xcp23 values around the mean time between CP2 and CP3

mean_time_cp2_cp3 = df['time_/min'].iloc[cp2:cp3+1].mean()
data_pca = data_pca.apply(lambda x: mean_time_cp2_cp3 * x)

# This data is usable as input training data for this particular measurement
# Complete training data can be generated by iteratively following same procedure for all the measurements
